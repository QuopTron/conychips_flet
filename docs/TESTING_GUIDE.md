# ğŸ§ª Sistema de Testing y Benchmarking

## ğŸ“‹ DescripciÃ³n

Sistema unificado de testing con 3 categorÃ­as de pruebas:
- **White Box**: Pruebas de caja blanca (estructura interna)
- **Black Box**: Pruebas de caja negra (comportamiento externo)
- **Benchmark**: MediciÃ³n de performance

## ğŸš€ Inicio RÃ¡pido

```bash
# Ejecutar TODOS los tests
./run_tests.sh all

# Solo white box
./run_tests.sh whitebox

# Solo black box
./run_tests.sh blackbox

# Solo benchmarks
./run_tests.sh benchmark

# Coverage completo
./run_tests.sh coverage
```

## ğŸ“‚ Estructura

```
tests/
â”œâ”€â”€ whitebox/              # Pruebas de caja blanca
â”‚   â””â”€â”€ test_layout_base.py
â”œâ”€â”€ blackbox/              # Pruebas de caja negra
â”‚   â””â”€â”€ test_layout_integration.py
â”œâ”€â”€ benchmark/             # Benchmarks de performance
â”‚   â””â”€â”€ benchmark_navigation.py
â””â”€â”€ reports/               # Reportes generados
    â”œâ”€â”€ whitebox-report.html
    â”œâ”€â”€ blackbox-report.html
    â””â”€â”€ coverage/
```

## ğŸ¯ Tipos de Pruebas

### White Box (Caja Blanca)
Conocen la implementaciÃ³n interna. Prueban:
- Herencia correcta de clases
- Atributos privados inicializados
- MÃ©todos internos funcionando
- Estructura de cÃ³digo

**Ejemplo:**
```python
def test_herencia_correcta(self):
    """Verifica que LayoutBase hereda de ft.Column"""
    assert issubclass(LayoutBase, ft.Column)
```

### Black Box (Caja Negra)
No conocen implementaciÃ³n. Prueban:
- Comportamiento externo
- Entrada/Salida
- IntegraciÃ³n entre componentes
- Casos extremos

**Ejemplo:**
```python
def test_construccion_basica(self):
    """Verifica que se puede construir LayoutBase"""
    layout = LayoutBase(pagina=page, usuario=usuario)
    assert layout is not None
```

### Benchmark (Performance)
Miden tiempos de ejecuciÃ³n:
- CreaciÃ³n de componentes
- Renderizado
- Operaciones complejas
- ComparaciÃ³n entre versiones

**Ejemplo:**
```python
def benchmark_layout_creation(self, iterations=100):
    start = time.perf_counter()
    for _ in range(iterations):
        layout = LayoutBase(...)
    end = time.perf_counter()
    return (end - start) / iterations * 1000  # ms
```

## ğŸ“Š Comandos Disponibles

### Ejecutar Tests

```bash
# Todos los tests
./run_tests.sh all

# White box solamente
./run_tests.sh whitebox

# Black box solamente
./run_tests.sh blackbox

# Benchmarks
./run_tests.sh benchmark
```

### Utilidades

```bash
# Limpiar reportes anteriores
./run_tests.sh clean

# Ver reportes disponibles
./run_tests.sh reports

# AnÃ¡lisis de cobertura
./run_tests.sh coverage

# Ayuda
./run_tests.sh help
```

### Prueba RÃ¡pida de Flujo

```bash
# Ejecuta prueba simple de imports y creaciÃ³n
python test_flujo_rapido.py
```

### Prueba de App Completa

```bash
# Inicia app y captura logs
./test_app.sh

# Ver logs en vivo
tail -f app_test_output.log

# Buscar errores
grep -i error app_test_output.log
```

## ğŸ“ˆ Benchmarking

### Ejecutar Benchmark

```bash
./run_tests.sh benchmark
```

### Salida Esperada

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      ğŸ”¥ BENCHMARK - Sistema de NavegaciÃ³n              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Iteraciones por test: 100

âœ“ LayoutBase Creation:        2.3450 ms/op
âœ“ construir() Method:          3.1230 ms/op
âœ“ NavbarGlobal Creation:       1.8970 ms/op
âœ“ BottomNavigation Creation:   2.5601 ms/op

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘ Total tiempo: 978.51 ms                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### Comparar Versiones

```python
from tests.benchmark.benchmark_navigation import NavigationBenchmark

bench = NavigationBenchmark()

# Resultados v1.0
results_v1 = bench.run_all_benchmarks(iterations=50)

# ... hacer cambios ...

# Resultados v1.1
results_v2 = bench.run_all_benchmarks(iterations=50)

# Comparar
bench.compare_versions(results_v1, results_v2)
```

## ğŸ” Coverage

### Ejecutar AnÃ¡lisis

```bash
./run_tests.sh coverage
```

### Ver Reporte HTML

```bash
# Se genera en htmlcov/index.html
xdg-open htmlcov/index.html  # Linux
open htmlcov/index.html      # macOS
```

### Interpretar Resultados

- **Verde**: CÃ³digo ejecutado (cubierto)
- **Rojo**: CÃ³digo no ejecutado (sin cobertura)
- **Meta**: >80% de cobertura

## ğŸ“ Escribir Nuevas Pruebas

### White Box Test

```python
# tests/whitebox/test_mi_componente.py
import pytest
from features.mi_modulo.MiClase import MiClase

class TestMiClaseWhiteBox:
    def test_atributo_privado(self):
        """Verifica que _atributo existe"""
        obj = MiClase()
        assert hasattr(obj, '_atributo')
```

### Black Box Test

```python
# tests/blackbox/test_mi_integracion.py
import pytest
from features.mi_modulo.MiClase import MiClase

class TestMiClaseBlackBox:
    def test_comportamiento_esperado(self):
        """Verifica salida correcta"""
        obj = MiClase()
        resultado = obj.hacer_algo(input="test")
        assert resultado == "esperado"
```

### Benchmark

```python
# tests/benchmark/benchmark_mi_modulo.py
import time
from features.mi_modulo.MiClase import MiClase

class MiBenchmark:
    def benchmark_operacion(self, iterations=100):
        start = time.perf_counter()
        for _ in range(iterations):
            MiClase().operacion_pesada()
        end = time.perf_counter()
        
        return (end - start) / iterations * 1000  # ms
```

## ğŸ› Debugging

### Ver Logs de Tests

```bash
# Tests con output detallado
pytest tests/whitebox/ -v -s

# Solo tests que fallan
pytest tests/ --tb=short --maxfail=1

# Tests especÃ­ficos
pytest tests/whitebox/test_layout_base.py::TestLayoutBaseWhiteBox::test_herencia_correcta -v
```

### Modo Interactivo

```bash
# Entrar en debugger al fallar
pytest --pdb

# Usar breakpoint en cÃ³digo
def test_algo():
    obj = MiClase()
    breakpoint()  # Se detiene aquÃ­
    assert obj.valor == 5
```

## ğŸ“š Mejores PrÃ¡cticas

### Naming

- **White Box**: `test_atributo_privado`, `test_metodo_interno`
- **Black Box**: `test_comportamiento_esperado`, `test_caso_extremo`
- **Benchmark**: `benchmark_operacion_compleja`

### OrganizaciÃ³n

```python
class TestMiClase:
    """Grupo de tests para MiClase"""
    
    def test_caso_1(self):
        """DescripciÃ³n clara del caso"""
        # Arrange
        obj = MiClase()
        
        # Act
        resultado = obj.hacer()
        
        # Assert
        assert resultado == esperado
```

### Fixtures

```python
import pytest

@pytest.fixture
def usuario_admin():
    """Usuario de prueba con rol admin"""
    return Usuario(ID=1, EMAIL="admin@test.com", ROLES=["ADMIN"])

def test_con_fixture(usuario_admin):
    assert usuario_admin.TIENE_ROL("ADMIN")
```

## ğŸ“Š CI/CD Integration

### GitHub Actions

```yaml
name: Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Setup Python
        uses: actions/setup-python@v2
      - name: Install Dependencies
        run: pip install -r requirements-test.txt
      - name: Run Tests
        run: ./run_tests.sh all
      - name: Upload Coverage
        uses: codecov/codecov-action@v2
```

## ğŸ¯ Objetivos de Calidad

- âœ… Cobertura de cÃ³digo > 80%
- âœ… Todos los tests pasando
- âœ… Performance estable entre versiones
- âœ… Sin regresiones detectadas

## ğŸ“ Soporte

Para problemas o preguntas:
1. Ver logs: `cat app_test_output.log`
2. Ejecutar prueba rÃ¡pida: `python test_flujo_rapido.py`
3. Verificar coverage: `./run_tests.sh coverage`

---

**Ãšltima actualizaciÃ³n:** 28 Enero 2026
**VersiÃ³n:** 1.0.0
